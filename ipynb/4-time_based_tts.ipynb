{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "# %matplotlib inline\n",
    "import pickle\n",
    "# from time import sleep\n",
    "from tqdm import tqdm\n",
    "# import seaborn as sns\n",
    "# import sqlite3\n",
    "# con = sqlite3.connect(\"../lastfm_1k_sql\")\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "X_train_2007 = pickle.load( open( \"X_train_2007\", \"rb\" ) ) \n",
    "X_test_2008 = pickle.load( open( \"X_test_2008\", \"rb\" ) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sidequest to build avg plays per user\n",
    "\n",
    "# for usersha1 in a list with all the users:\n",
    "#     # pull all artists/plays for a given user\n",
    "#     sql = (\"SELECT lastfm_360k_sql.usersha1, lastfm_360k_sql.artmbid, lastfm_360k_sql.artname, lastfm_360k_sql.plays \"\n",
    "#            \"FROM lastfm_360k_sql \"\n",
    "#            \"WHERE lastfm_360k_sql.usersha1 = '{0}' \").format(usersha)\n",
    "#     user1 = pd.read_sql(sql, con=con)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build our train and test sets from 2007 and 2008 data\n",
    "- Note that this code does not run off of github because it relies on a 4.5gb data file that GitHub cannot host.\n",
    "- The general process followed below is to load Last.FM data for a user in two chunks - one in 2007, one in 2008.  These are our train/test splits which let us compare whether our top recommendations for a user in 2007 appear in their listened genres in 2008."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select n random userids from lastfm_1k\n",
    "def select_random_users(num_users):\n",
    "    sql = (\"SELECT DISTINCT userid \"\n",
    "           \"FROM lastfm_1k_sql \"\n",
    "           \"ORDER BY Random() \"\n",
    "           \"LIMIT'{0}'\").format(num_users)\n",
    "\n",
    "    user_list = pd.read_sql(sql, con=con)\n",
    "    return user_list\n",
    "\n",
    "# build a unit vector genre list\n",
    "def select_unique_genres():\n",
    "    sql = (\"SELECT DISTINCT genre \"\n",
    "           \"FROM artist_genre_percent_sql \"\n",
    "           )\n",
    "\n",
    "    genre_matrix = pd.read_sql(sql, con=con)\n",
    "    return pd.DataFrame(genre_matrix, index=genre_matrix['genre']).T\n",
    "\n",
    "genre_vector = select_unique_genres()\n",
    "\n",
    "# pull all artists/plays for a given user\n",
    "def artists_time(userid):\n",
    "    sql = (\"SELECT timestamp, artid \"\n",
    "           \"FROM lastfm_1k_sql \"\n",
    "           \"WHERE userid = '{0}' \").format(userid)\n",
    "    user1 = pd.read_sql(sql, con=con)\n",
    "    return user1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset by user-defined year (through may 5 2009)\n",
    "# default should be comparing 2007 and 2008\n",
    "\n",
    "def build_user_vector(userid, year1, test_values=False, full_matrix=False, scale_by_plays=True):\n",
    "    print(userid)\n",
    "    sql1 = (\"SELECT artid, COUNT(lastfm_1k_sql.artid) AS plays \"\n",
    "           \"FROM lastfm_1k_sql \"\n",
    "           \"WHERE userid = '{0}' AND \"\n",
    "           \"(timestamp >= '{1}-01-01T00:00:00.000' AND \"\n",
    "           \"timestamp <= '{1}-12-31T23:59:59.999')\"\n",
    "           \"GROUP BY artid \").format(userid, year1)\n",
    "    user1 = pd.read_sql(sql1, con=con)\n",
    "    \n",
    "    # error check in case no data is returned for a given user in that time period\n",
    "    if user1.shape == (0,2):\n",
    "        return None\n",
    "    \n",
    "    # pull the user's artist/genre mix\n",
    "    sql2 = (\"SELECT artist_genre_percent_sql.artmbid as id, \"\n",
    "           \"artist_genre_percent_sql.artname, \"\n",
    "           \"artist_genre_percent_sql.genre, \"\n",
    "           \"artist_genre_percent_sql.genre_percent \"\n",
    "           \"FROM artist_genre_percent_sql \"\n",
    "           \"WHERE id in (SELECT artid \"\n",
    "           \"FROM lastfm_1k_sql \"\n",
    "           \"WHERE userid = '{0}' AND \"\n",
    "           \"(timestamp >= '{1}-01-01T00:00:00.000' AND \"\n",
    "           \"timestamp <= '{1}-12-31T23:59:59.999')\"\n",
    "           \"GROUP BY artid )\").format(userid, year1)\n",
    "    artist_genre = pd.read_sql(sql2, con=con)\n",
    "    artist_genre.rename(columns={'id':'artmbid'}, inplace=True)\n",
    "    \n",
    "    # error check in case no data is returned for a given user in that time period\n",
    "    if artist_genre.shape == (0,4):\n",
    "        return None\n",
    "    \n",
    "    # pivot the matrix w/ genres\n",
    "    joined_matrix = artist_genre.pivot_table(values='genre_percent',\n",
    "                                                index='artmbid', \n",
    "                                                columns='genre', \n",
    "                                                dropna=False)\n",
    "\n",
    "    # scale the user's genre preference by # of plays of that artist\n",
    "    if scale_by_plays:\n",
    "        for artmbid1 in user1.artid.values:\n",
    "            num_plays = user1.plays[user1.artid == artmbid1].values\n",
    "            try:\n",
    "                joined_matrix.loc[artmbid1] *= num_plays\n",
    "            except KeyError:\n",
    "                pass\n",
    "    \n",
    "    # testing code. these should be left false for production\n",
    "    if test_values:\n",
    "        joined_matrix['e'] = joined_matrix.sum(axis=1)\n",
    "        return joined_matrix['e'][~joined_matrix['e'].isnull()]\n",
    "    if full_matrix:\n",
    "        return joined_matrix\n",
    "\n",
    "    else:\n",
    "        # return a dictionary with total genre profile of the user, indexed by usersha1\n",
    "        joined_matrix.loc[userid] = joined_matrix.sum()\n",
    "        user_vector = joined_matrix.loc[userid]\n",
    "        return user_vector.to_dict()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build genre taste matrix for specified number of random users w/ a train/test split\n",
    "\n",
    "def build_taste_matrix(num_users, year1, year2):\n",
    "    \n",
    "    user_list_of_dicts_train = []\n",
    "    index_list_train = []\n",
    "    \n",
    "    user_list_of_dicts_test = []\n",
    "    index_list_test = []\n",
    "    \n",
    "    how_many = select_random_users(num_users)\n",
    "    \n",
    "    # build user vectors for each user. if no data is available in a year, skip\n",
    "    for i in tqdm(range(len(how_many))):\n",
    "        useri_vector = build_user_vector(how_many['userid'][i], year1)\n",
    "        if useri_vector != None:\n",
    "            user_list_of_dicts_train.append(useri_vector)\n",
    "            index_list_train.append(how_many['userid'][i])\n",
    "    X_train = pd.DataFrame(user_list_of_dicts_train, index=index_list_train)\n",
    "    \n",
    "    for i in tqdm(range(len(how_many))):\n",
    "        useri_vector = build_user_vector(how_many['userid'][i], year2)\n",
    "        if useri_vector != None:\n",
    "            user_list_of_dicts_test.append(useri_vector)\n",
    "            index_list_test.append(how_many['userid'][i])\n",
    "    X_test = pd.DataFrame(user_list_of_dicts_test, index=index_list_test)\n",
    "    \n",
    "    # remove users that didn't listen to music in BOTH years\n",
    "    train_index = set(X_train.index)\n",
    "    test_index = set(X_test.index)\n",
    "    final_columns = list(train_index & test_index)\n",
    "    X_train = X_train[X_train.index.isin(final_columns)].fillna(0)\n",
    "    X_test = X_test[X_test.index.isin(final_columns)].fillna(0)\n",
    "\n",
    "    # name the index\n",
    "    X_train.index.name = 'userid'\n",
    "    X_test.index.name = 'userid'\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_2007, X_test_2008 = build_taste_matrix(800, 2007, 2008)\n",
    "# pickle.dump(X_train_2007, open('X_train_2007', \"wb\" ) )\n",
    "# pickle.dump(X_test_2008, open('X_test_2008', \"wb\" ) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run recommendations for everyone in our train set and check if they appear in test set\n",
    "### Cosine Similarity w/ weighted recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull all the relevant information for a random user or a user w/ given userid\n",
    "def define_user(train, test, random=True, userid=None):\n",
    "    \n",
    "    # sample 1 user from our list\n",
    "    if random:\n",
    "        random_user = train.sample()\n",
    "        random_usersha1 = random_user.index[0]\n",
    "        random_user_test = test.loc[random_usersha1]\n",
    "    else:\n",
    "        random_user = train.loc[userid]\n",
    "        random_user_test = test.loc[userid]\n",
    "        random_usersha1 = userid\n",
    "    \n",
    "    # our user's vector and top genres for train and test sets\n",
    "    vector = train.loc[random_usersha1][train.loc[random_usersha1] > 0]\\\n",
    "        .sort_values(ascending=False)\n",
    "    vector_test = test.loc[random_usersha1][test.loc[random_usersha1] > 0]\\\n",
    "        .sort_values(ascending=False)\n",
    "    return random_usersha1, random_user, vector, random_user_test, vector_test\n",
    "\n",
    "# define_user(X_train_2007, X_test_2008)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a similarity matrix, select our random user, \n",
    "# remove genres the user has listened to, and output recommendations\n",
    "\n",
    "def cosine_recommendations(no_accounting_for_taste, random_usersha1):\n",
    "\n",
    "    users_sim = pd.DataFrame(cosine_similarity(no_accounting_for_taste, no_accounting_for_taste), \n",
    "                             columns=no_accounting_for_taste.index, \n",
    "                             index=no_accounting_for_taste.index)\n",
    "    sim_user = users_sim[random_usersha1]\n",
    "    user_mask = no_accounting_for_taste.loc[random_usersha1] < 0.01 # anything with zero hasn't been listened to\n",
    "\n",
    "    user_genre_matrix = no_accounting_for_taste.loc[:, user_mask] # remove genres user has listened to\n",
    "    user_genre_matrix = user_genre_matrix.multiply(sim_user, axis='rows') # weight genres by user similarity \n",
    "    user_genre_matrix = user_genre_matrix.drop(random_usersha1, axis = 0) # drop user from own recs\n",
    "    user_recommend = user_genre_matrix.sum().sort_values(ascending=False) # sum genre totals and order\n",
    "    return user_recommend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jaccard function to compare our target users' train and test sets\n",
    "def jaccard(train_recs, test_vecs):\n",
    "    \n",
    "    # standard jaccard setup\n",
    "    a = set(train_recs.index)\n",
    "    b = set(test_vecs.index)\n",
    "    numerator = a.intersection(b)\n",
    "    denominator = a.union(b)\n",
    "    diff_items = len(b.difference(a))\n",
    "    \n",
    "    # if our number one train recommendation is the number one result in test set, return 1\n",
    "    # error check here eliminates users with very low listens (<20 genres in a year)\n",
    "    try:\n",
    "        if test_vecs.head(20).index[0] == train_recs.head(20).index[0]:\n",
    "            return 1, diff_items\n",
    "    except IndexError:\n",
    "        pass\n",
    "    # otherwise, return the score and the number of dissimilar items out of 20\n",
    "    return len(numerator)/len(denominator), diff_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_cos_recs(X_train_2007, X_test_2008):\n",
    "    \n",
    "    # initiate a blank dict to hold our results\n",
    "    results_list = {}\n",
    "    \n",
    "    # loop over each user in the train set\n",
    "    for user in tqdm(X_train_2007.index):\n",
    "        \n",
    "        # build the rec for a given user\n",
    "        cos_recs_2007 = cosine_recommendations(X_train_2007, user)\n",
    "        \n",
    "        # pull other listening information for the given user\n",
    "        random_usersha1, random_user, vector, random_user_test, vector_test = \\\n",
    "            define_user(X_train_2007, X_test_2008, random=False, userid=user)\n",
    "        \n",
    "        # compare 2007 recommendations ( = 2007 recs - 2007 listened) \n",
    "        # to 2008 new ( = 2008  listened - 2007 listened) to see if new == recommended\n",
    "\n",
    "        test_mask = ~vector_test.index.isin(vector.index) # filter out 2007 listened from 2008 listened\n",
    "        jaccard_results = jaccard(cos_recs_2007.head(20), vector_test[test_mask].head(20))\n",
    "        results_list[user] = jaccard_results\n",
    "        \n",
    "        results_df = pd.DataFrame(results_list).T\n",
    "        results_df.rename(columns={0:'jaccard',1:'diff items'}, inplace=True)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_list = eval_cos_recs(X_train_2007, X_test_2008)\n",
    "# results_df = pd.DataFrame(results_list).T\n",
    "# results_df.rename(columns={0:'jaccard',1:'diff items'}, inplace=True)\n",
    "# pickle.dump(results_df, open('cos_results_df', \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cos_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hit ratio'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.5617977528089888"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'bullseye ratio'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.0149812734082397"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'bullseyes'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cos_results_df = pickle.load( open( \"cos_results_df\", \"rb\" ) ) \n",
    "\n",
    "hit_ratio = len(cos_results_df[cos_results_df['jaccard'] > 0]) / len(cos_results_df)\n",
    "display(\"hit ratio\", hit_ratio)\n",
    "\n",
    "# number of bullseye hits out of 534\n",
    "bullseye_ratio = len(cos_results_df[cos_results_df['jaccard'] == 1]) / len(cos_results_df)\n",
    "display(\"bullseye ratio\", bullseye_ratio)\n",
    "\n",
    "bullseyes = len(cos_results_df[cos_results_df['jaccard'] == 1])\n",
    "display(\"bullseyes\", bullseyes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD w/ Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return all genres from a given user with the given threshold\n",
    "def get_genres(row, no_accounting_for_taste, genre_thresh):\n",
    "    genre_list = []\n",
    "    for column in no_accounting_for_taste.columns:\n",
    "        if no_accounting_for_taste.loc[row['userid']][column] > genre_thresh:\n",
    "            genre_list.append(column)\n",
    "    return genre_list\n",
    "\n",
    "# return all genres from a given user that they ever listened to\n",
    "def get_all_genres(userid, no_accounting_for_taste):\n",
    "    genre_list = []\n",
    "    for column in no_accounting_for_taste.columns:\n",
    "        if no_accounting_for_taste.loc[userid][column] > 0:\n",
    "            genre_list.append(column)\n",
    "    return genre_list\n",
    "\n",
    "# look up a userid by index\n",
    "def user_lookup(user_index, no_accounting_for_taste):    \n",
    "    return no_accounting_for_taste.iloc[user_index].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reducing from 11,188 genres down to 40 components still yields 97% variance explanation\n",
    "# clearly, there are many genres that overlap, \n",
    "# and this should reduce some of that dimensionality\n",
    "\n",
    "def SVD_results(no_accounting_for_taste, random_user, user_thresh, genre_thresh):\n",
    "    n_components = 40\n",
    "    SVD = TruncatedSVD(n_components)\n",
    "    component_names = [\"component_\"+str(i+1) for i in range(n_components)]\n",
    "    svd_matrix = SVD.fit_transform(no_accounting_for_taste)\n",
    "\n",
    "    # transform the user's profile to the SVD and df our full user matrix\n",
    "    svd_user = SVD.transform(random_user) \n",
    "    svd_df = pd.DataFrame(svd_matrix)\n",
    "\n",
    "    # add a cosine similarity column between the random user and the full user list\n",
    "    # at the moment this similarity takes into account ALL genres, even low-value ones. those are filtered out after \n",
    "    # similarity is calculated.  this likely affects who is similar, but only marginally\n",
    "    svd_df['cosine_sim'] = cosine_similarity(svd_df, svd_user) \n",
    "\n",
    "    # pull the 10 most similar users.  note: 0 = our user\n",
    "    results_matrix = svd_df[['cosine_sim']].sort_values('cosine_sim', ascending=False)[0:10] \n",
    "\n",
    "    try:\n",
    "        # look up userids, add them, and append a list of their genres\n",
    "        results_matrix['userid'] = results_matrix.index\n",
    "        results_matrix['userid'] = results_matrix['userid'].apply(lambda x: user_lookup(x, no_accounting_for_taste))\n",
    "        results_matrix['user_genres'] = results_matrix.apply(lambda x: get_genres(x, no_accounting_for_taste, genre_thresh), axis=1)\n",
    "    except ValueError:\n",
    "        return ['nothing here']\n",
    "    \n",
    "    # to reduce variance, we can compare the other user curated lists to the entire random_user_list\n",
    "    # this should eliminate us recommending a genre to someone who has listened to it a little bit, below the threshold\n",
    "\n",
    "    # pull the random user's full list, and everyone else in the top <<user_thresh>>\n",
    "    random_user_full_list = set(get_all_genres(random_user.name, no_accounting_for_taste))\n",
    "    all_other_list = set(results_matrix['user_genres'][1:user_thresh].sum())\n",
    "\n",
    "    # all items appearing in our similar users' top lists and not in our random user's full list\n",
    "    diff_full_list = all_other_list.difference(random_user_full_list)\n",
    "\n",
    "    return diff_full_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jaccard function to compare our target user's train and test set\n",
    "def jaccard_svd(train_recs, test_vecs):\n",
    "    \n",
    "    # standard jaccard setup\n",
    "    a = set(train_recs)\n",
    "    b = set(test_vecs.index)\n",
    "    numerator = a.intersection(b)\n",
    "    denominator = a.union(b)\n",
    "    diff_items = len(b.difference(a))\n",
    "    \n",
    "    # return the score and the number of dissimilar items out of len(2008 listens),\n",
    "    # with error check for no listens\n",
    "    try:\n",
    "        jaccard_score = len(numerator)/len(denominator)\n",
    "    except ZeroDivisionError:\n",
    "        jaccard_score = 0\n",
    "    \n",
    "    return jaccard_score, diff_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_svd_recs(X_train_2007, X_test_2008, user_threshold, genre_threshold):\n",
    "    \n",
    "    # initiate a blank dict to hold our results\n",
    "    results_list = {}\n",
    "    \n",
    "    # loop over each user in the train set\n",
    "    for user in tqdm(X_train_2007.index):\n",
    "        \n",
    "        # pull other listening information for the given user\n",
    "        random_usersha1, random_user, vector, random_user_test, vector_test = \\\n",
    "            define_user(X_train_2007, X_test_2008, random=False, userid=user)\n",
    "        \n",
    "        # build the rec for a given user\n",
    "        svd_recs_2007 = SVD_results(X_train_2007, random_user, user_thresh=user_threshold, genre_thresh=genre_threshold)\n",
    "        \n",
    "        # compare 2007 recommendations (which = 2007 recs - 2007 listened) \n",
    "        # to 2008 new (2008  listened - 2007 listened) to see if new == recommended\n",
    "\n",
    "        test_mask = ~vector_test.index.isin(vector.index) # filter out 2007 listened from 2008 listened\n",
    "        jaccard_results = jaccard_svd(svd_recs_2007, vector_test[test_mask])\n",
    "        results_list[user] = jaccard_results\n",
    "        \n",
    "#         results_df = pd.DataFrame(results_list).T\n",
    "#         results_df.rename(columns={0:'jaccard',1:'diff items'}, inplace=True)\n",
    "#     return results_df\n",
    "    return results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set threshold for eliminating \"bad\" genres.  this is essentially our hyperparameter that filters for the \n",
    "# weighted number we've been carrying forward on our matrices, ranging from 0 to hundreds\n",
    "# a threshold of 50 could mean 50 listens to an artist with 1 tag, or 100 listens to an artist with 2 equal tags, etc\n",
    "\n",
    "# grid search over user_thresh=5, 10\n",
    "# genre_thresh = 20, 30, 40, 50\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_dict1 = {}\n",
    "# for user_thresh in (5,10):\n",
    "#     for genre_thresh in (20,30):\n",
    "#         results_dict1[(user_thresh,genre_thresh)] = eval_svd_recs(\n",
    "#                                                         X_train_2007, \n",
    "#                                                         X_test_2008, \n",
    "#                                                         user_thresh, \n",
    "#                                                         genre_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(results_dict1, open('results_dict1', \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_dict2 = {}\n",
    "# for user_thresh in (5,10):\n",
    "#     for genre_thresh in (40,50):\n",
    "#         results_dict2[(user_thresh,genre_thresh)] = eval_svd_recs(\n",
    "#                                                         X_train_2007, \n",
    "#                                                         X_test_2008, \n",
    "#                                                         user_thresh, \n",
    "#                                                         genre_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(results_dict2, open('results_dict2', \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 534/534 [6:49:47<00:00, 46.04s/it]  \n",
      "100%|██████████| 534/534 [6:57:10<00:00, 46.87s/it]  \n"
     ]
    }
   ],
   "source": [
    "# user_thresh = 10\n",
    "\n",
    "# results_dict11 = {}\n",
    "# for genre_thresh in (20,30):\n",
    "#     results_dict11[(user_thresh,genre_thresh)] = eval_svd_recs(\n",
    "#                                                         X_train_2007, \n",
    "#                                                         X_test_2008, \n",
    "#                                                         user_thresh, \n",
    "#                                                         genre_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(results_dict11, open('results_dict11', \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 83/534 [1:05:05<5:53:40, 47.05s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-3a02eb5ff4c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m                                                         \u001b[0mX_test_2008\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                                                         \u001b[0muser_thresh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                                                         genre_thresh)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-ee1e0b581f22>\u001b[0m in \u001b[0;36meval_svd_recs\u001b[0;34m(X_train_2007, X_test_2008, user_threshold, genre_threshold)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# build the rec for a given user\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0msvd_recs_2007\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVD_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_2007\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_user\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_thresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_threshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenre_thresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenre_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# compare 2007 recommendations (which = 2007 recs - 2007 listened)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-1541343ce686>\u001b[0m in \u001b[0;36mSVD_results\u001b[0;34m(no_accounting_for_taste, random_user, user_thresh, genre_thresh)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mresults_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'userid'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mresults_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'userid'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'userid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0muser_lookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_accounting_for_taste\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mresults_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user_genres'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_genres\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_accounting_for_taste\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenre_thresh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'nothing here'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, broadcast, raw, reduce, args, **kwds)\u001b[0m\n\u001b[1;32m   4150\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4151\u001b[0m                         \u001b[0mreduce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4152\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4153\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4154\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_broadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_apply_standard\u001b[0;34m(self, func, axis, ignore_failures, reduce)\u001b[0m\n\u001b[1;32m   4206\u001b[0m                     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_agg_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4207\u001b[0m                     result = lib.reduce(values, func, axis=axis, dummy=dummy,\n\u001b[0;32m-> 4208\u001b[0;31m                                         labels=labels)\n\u001b[0m\u001b[1;32m   4209\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4210\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/src/reduce.pyx\u001b[0m in \u001b[0;36mpandas.lib.reduce (pandas/lib.c:45030)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/src/reduce.pyx\u001b[0m in \u001b[0;36mpandas.lib.Reducer.get_result (pandas/lib.c:34673)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-1541343ce686>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mresults_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'userid'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mresults_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'userid'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'userid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0muser_lookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_accounting_for_taste\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mresults_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user_genres'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_genres\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_accounting_for_taste\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenre_thresh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'nothing here'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-bfe4ed68552d>\u001b[0m in \u001b[0;36mget_genres\u001b[0;34m(row, no_accounting_for_taste, genre_thresh)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mgenre_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mno_accounting_for_taste\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mno_accounting_for_taste\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'userid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mgenre_thresh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m             \u001b[0mgenre_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgenre_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1310\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1312\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1481\u001b[0m         \u001b[0;31m# fall thru to straight lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_valid_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1483\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_label\u001b[0;34m(self, label, axis)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'no slices here, handle elsewhere'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mxs\u001b[0;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[1;32m   1802\u001b[0m             result = self._constructor_sliced(\n\u001b[1;32m   1803\u001b[0m                 \u001b[0mnew_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1804\u001b[0;31m                 name=self.index[loc], dtype=new_values.dtype)\n\u001b[0m\u001b[1;32m   1805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1806\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m                 data = _sanitize_array(data, index, dtype, copy,\n\u001b[0;32m--> 243\u001b[0;31m                                        raise_cast_failure=True)\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSingleBlockManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_sanitize_array\u001b[0;34m(data, index, dtype, copy, raise_cast_failure)\u001b[0m\n\u001b[1;32m   2856\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2858\u001b[0;31m             \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2860\u001b[0m             \u001b[0;31m# possibility of nan -> garbage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# user_thresh = 5\n",
    "\n",
    "# results_dict22 = {}\n",
    "# for genre_thresh in (40,50):\n",
    "#     results_dict22[(user_thresh,genre_thresh)] = eval_svd_recs(\n",
    "#                                                         X_train_2007, \n",
    "#                                                         X_test_2008, \n",
    "#                                                         user_thresh, \n",
    "#                                                         genre_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(results_dict22, open('results_dict22', \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the ratios from each of my dataframes\n",
    "\n",
    "def gimme_ratios(svd_results_dict):\n",
    "    results_df = pd.DataFrame(svd_results_dict).T\n",
    "    results_df.rename(columns={0:'jaccard',1:'diff items'}, inplace=True)\n",
    "    hit_ratio = len(results_df[results_df['jaccard'] > 0]) / len(results_df)\n",
    "    return results_df, hit_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h1020</th>\n",
       "      <th>h1030</th>\n",
       "      <th>h1040</th>\n",
       "      <th>h1050</th>\n",
       "      <th>h540</th>\n",
       "      <th>h550</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hit_ratio</th>\n",
       "      <td>0.696629</td>\n",
       "      <td>0.59176</td>\n",
       "      <td>0.511236</td>\n",
       "      <td>0.432584</td>\n",
       "      <td>0.314607</td>\n",
       "      <td>0.2397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              h1020    h1030     h1040     h1050      h540    h550\n",
       "hit_ratio  0.696629  0.59176  0.511236  0.432584  0.314607  0.2397"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dict11 = pickle.load( open( \"results_dict11\", \"rb\" ) ) \n",
    "results_dict2 = pickle.load( open( \"results_dict2\", \"rb\" ) ) \n",
    "\n",
    "# df520, hit_ratio520 = gimme_ratios(results_dict1[(5,20)])\n",
    "# df530, hit_ratio530 = gimme_ratios(results_dict1[(5,30)])\n",
    "df540, hit_ratio540 = gimme_ratios(results_dict2[(5,40)])\n",
    "df550, hit_ratio550 = gimme_ratios(results_dict2[(5,50)])\n",
    "df1020, hit_ratio1020 = gimme_ratios(results_dict11[(10,20)])\n",
    "df1030, hit_ratio1030 = gimme_ratios(results_dict11[(10,30)])\n",
    "df1040, hit_ratio1040 = gimme_ratios(results_dict2[(10,40)])\n",
    "df1050, hit_ratio1050 = gimme_ratios(results_dict2[(10,50)])\n",
    "\n",
    "d = {\n",
    "# df520: hit_ratio520,\n",
    "# df530: hit_ratio530,\n",
    "'h540': hit_ratio540,\n",
    "'h550': hit_ratio550,\n",
    "'h1020': hit_ratio1020,\n",
    "'h1030': hit_ratio1030,\n",
    "'h1040': hit_ratio1040,\n",
    "'h1050': hit_ratio1050\n",
    "}\n",
    "df = pd.DataFrame(data=d, index=['hit_ratio'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
